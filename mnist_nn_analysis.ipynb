{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "training_data = list(training_data)\n",
    "validation_data = list(validation_data)\n",
    "test_data = list(test_data)\n",
    "\n",
    "train_inputs = []\n",
    "train_labels = []\n",
    "\n",
    "\n",
    "for i in training_data:\n",
    "    train_inputs.append(i[0])\n",
    "    x = list(i[1])\n",
    "    y = x.index(1)\n",
    "    train_labels.append(y)\n",
    "\n",
    "\n",
    "test_inputs = []\n",
    "test_labels = []\n",
    "\n",
    "for i in test_data:\n",
    "    test_inputs.append(i[0])\n",
    "    test_labels.append(i[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_15 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109386 (427.29 KB)\n",
      "Trainable params: 109386 (427.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 2.2151 - accuracy: 0.3322\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8581 - accuracy: 0.6016\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2910 - accuracy: 0.7047\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.9339 - accuracy: 0.7704\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.7494 - accuracy: 0.8122\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.6341 - accuracy: 0.8393\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.5557 - accuracy: 0.8575\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.5004 - accuracy: 0.8694\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.4602 - accuracy: 0.8777\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4297 - accuracy: 0.8840\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3970 - accuracy: 0.8939\n",
      "Test Loss: 0.3969677686691284\n",
      "Test Accuracy: 0.8938999772071838\n"
     ]
    }
   ],
   "source": [
    "# Initial Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "my_nn_model = keras.Sequential()\n",
    "my_nn_model.add(keras.layers.Flatten(input_shape=(784, 1)))\n",
    "my_nn_model.add(keras.layers.Dense(128, activation='sigmoid'))\n",
    "my_nn_model.add(keras.layers.Dense(64, activation='sigmoid'))\n",
    "my_nn_model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "my_nn_model.summary()\n",
    "\n",
    "\n",
    "my_nn_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "# sparse_categorical_crossentropy is used as the loss function since the classified outputs of the mode (0-9) are mutually exclusive\n",
    "\n",
    "\n",
    "results = my_nn_model.fit(np.array(train_inputs), np.array(train_labels), epochs=10)\n",
    "\n",
    "test_loss, test_acc = my_nn_model.evaluate(np.array(test_inputs), np.array(test_labels))\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_14 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109386 (427.29 KB)\n",
      "Trainable params: 109386 (427.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3967 - accuracy: 0.8901\n",
      "Epochs trained: 10\n",
      "Test Loss: 0.396740585565567\n",
      "Test Accuracy: 0.8901000022888184\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2550 - accuracy: 0.9269\n",
      "Epochs trained: 20\n",
      "Test Loss: 0.2550284266471863\n",
      "Test Accuracy: 0.9269000291824341\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1514 - accuracy: 0.9553\n",
      "Epochs trained: 50\n",
      "Test Loss: 0.1514463573694229\n",
      "Test Accuracy: 0.955299973487854\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9720\n",
      "Epochs trained: 100\n",
      "Test Loss: 0.09122510254383087\n",
      "Test Accuracy: 0.972000002861023\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.9790\n",
      "Epochs trained: 250\n",
      "Test Loss: 0.07068752497434616\n",
      "Test Accuracy: 0.9789999723434448\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9799\n",
      "Epochs trained: 500\n",
      "Test Loss: 0.07714104652404785\n",
      "Test Accuracy: 0.9799000024795532\n"
     ]
    }
   ],
   "source": [
    "# Changing number of epochs\n",
    "# Would expect overfitting as epochs increase\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "my_nn_model = keras.Sequential()\n",
    "my_nn_model.add(keras.layers.Flatten(input_shape=(784, 1)))\n",
    "my_nn_model.add(keras.layers.Dense(128, activation='sigmoid'))\n",
    "my_nn_model.add(keras.layers.Dense(64, activation='sigmoid'))\n",
    "my_nn_model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "my_nn_model.summary()\n",
    "\n",
    "my_nn_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "            optimizer=\"sgd\",\n",
    "            metrics=[\"accuracy\"])\n",
    "# sparse_categorical_crossentropy is used as the loss function since the classified outputs of the mode (0-9) are mutually exclusive\n",
    "\n",
    "\n",
    "num_epochs = [10, 20, 50, 100, 250, 500]\n",
    "\n",
    "for i in num_epochs:\n",
    "    results = my_nn_model.fit(np.array(train_inputs), np.array(train_labels), epochs=i, verbose=False)\n",
    "\n",
    "    test_loss, test_acc = my_nn_model.evaluate(np.array(test_inputs), np.array(test_labels))\n",
    "    print(\"Epochs trained:\", i)\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes in first hidden layer: 512  Nodes in second hidden layer 512\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3578 - accuracy: 0.8976\n",
      "Test Loss: 0.3577618896961212\n",
      "Test Accuracy: 0.897599995136261\n",
      "Nodes in first hidden layer: 512  Nodes in second hidden layer 256\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3566 - accuracy: 0.8989\n",
      "Test Loss: 0.3565708100795746\n",
      "Test Accuracy: 0.8988999724388123\n",
      "Nodes in first hidden layer: 512  Nodes in second hidden layer 128\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3609 - accuracy: 0.8978\n",
      "Test Loss: 0.36094585061073303\n",
      "Test Accuracy: 0.8978000283241272\n",
      "Nodes in first hidden layer: 512  Nodes in second hidden layer 64\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3756 - accuracy: 0.8955\n",
      "Test Loss: 0.3756021559238434\n",
      "Test Accuracy: 0.8955000042915344\n",
      "Nodes in first hidden layer: 512  Nodes in second hidden layer 32\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3941 - accuracy: 0.8946\n",
      "Test Loss: 0.3941355049610138\n",
      "Test Accuracy: 0.894599974155426\n",
      "Nodes in first hidden layer: 256  Nodes in second hidden layer 256\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3649 - accuracy: 0.8984\n",
      "Test Loss: 0.3649265766143799\n",
      "Test Accuracy: 0.8984000086784363\n",
      "Nodes in first hidden layer: 256  Nodes in second hidden layer 128\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3680 - accuracy: 0.8966\n",
      "Test Loss: 0.3679578900337219\n",
      "Test Accuracy: 0.8966000080108643\n",
      "Nodes in first hidden layer: 256  Nodes in second hidden layer 64\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3789 - accuracy: 0.8962\n",
      "Test Loss: 0.3788999915122986\n",
      "Test Accuracy: 0.8962000012397766\n",
      "Nodes in first hidden layer: 256  Nodes in second hidden layer 32\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4012 - accuracy: 0.8927\n",
      "Test Loss: 0.40124455094337463\n",
      "Test Accuracy: 0.8927000164985657\n",
      "Nodes in first hidden layer: 128  Nodes in second hidden layer 128\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3861 - accuracy: 0.8925\n",
      "Test Loss: 0.3861483633518219\n",
      "Test Accuracy: 0.8924999833106995\n",
      "Nodes in first hidden layer: 128  Nodes in second hidden layer 64\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.4046 - accuracy: 0.8920\n",
      "Test Loss: 0.4045702815055847\n",
      "Test Accuracy: 0.8920000195503235\n",
      "Nodes in first hidden layer: 128  Nodes in second hidden layer 32\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4364 - accuracy: 0.8863\n",
      "Test Loss: 0.4363630414009094\n",
      "Test Accuracy: 0.8863000273704529\n",
      "Nodes in first hidden layer: 64  Nodes in second hidden layer 64\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4154 - accuracy: 0.8900\n",
      "Test Loss: 0.41544678807258606\n",
      "Test Accuracy: 0.8899999856948853\n",
      "Nodes in first hidden layer: 64  Nodes in second hidden layer 32\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4657 - accuracy: 0.8836\n",
      "Test Loss: 0.465740829706192\n",
      "Test Accuracy: 0.8835999965667725\n",
      "Nodes in first hidden layer: 32  Nodes in second hidden layer 32\n",
      "313/313 [==============================] - 0s 894us/step - loss: 0.5082 - accuracy: 0.8644\n",
      "Test Loss: 0.5082046985626221\n",
      "Test Accuracy: 0.8644000291824341\n"
     ]
    }
   ],
   "source": [
    "# Changing number of hidden units in each layer\n",
    "\n",
    "hiddenLayer_numNodes = [512, 256, 128, 64, 32]\n",
    "\n",
    "for i in range(len(hiddenLayer_numNodes)):\n",
    "    for j in range(i, len(hiddenLayer_numNodes)):\n",
    "        print(\"Nodes in first hidden layer:\", hiddenLayer_numNodes[i], \" Nodes in second hidden layer\", hiddenLayer_numNodes[j])\n",
    "\n",
    "        my_nn_model = keras.Sequential()\n",
    "        my_nn_model.add(keras.layers.Flatten(input_shape=(784, 1)))\n",
    "        my_nn_model.add(keras.layers.Dense(hiddenLayer_numNodes[i], activation='sigmoid'))\n",
    "        my_nn_model.add(keras.layers.Dense(hiddenLayer_numNodes[j], activation='sigmoid'))\n",
    "        my_nn_model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "        # my_nn_model.summary()\n",
    "\n",
    "        my_nn_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "            optimizer=\"sgd\",\n",
    "            metrics=[\"accuracy\"])\n",
    "        # sparse_categorical_crossentropy is used as the loss function since the classified outputs of the mode (0-9) are mutually exclusive\n",
    "\n",
    "        results = my_nn_model.fit(np.array(train_inputs), np.array(train_labels), epochs=10, verbose=False)\n",
    "\n",
    "        test_loss, test_acc = my_nn_model.evaluate(np.array(test_inputs), np.array(test_labels))\n",
    "        print(\"Test Loss:\", test_loss)\n",
    "        print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: <keras.src.initializers.initializers.RandomNormal object at 0x143f12050>\n",
      "Kernel: <keras.src.initializers.initializers.RandomNormal object at 0x180aba450>\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4385 - accuracy: 0.8794\n",
      "Test Loss: 0.4385152757167816\n",
      "Test Accuracy: 0.8794000148773193\n",
      "Bias: <keras.src.initializers.initializers.RandomNormal object at 0x143f12050>\n",
      "Kernel: <keras.src.initializers.initializers.Zeros object at 0x180abaf90>\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.7765 - accuracy: 0.2952\n",
      "Test Loss: 1.7765247821807861\n",
      "Test Accuracy: 0.295199990272522\n",
      "Bias: <keras.src.initializers.initializers.RandomNormal object at 0x143f12050>\n",
      "Kernel: <keras.src.initializers.initializers.Ones object at 0x180ab8490>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 2.3049 - accuracy: 0.1135\n",
      "Test Loss: 2.3049395084381104\n",
      "Test Accuracy: 0.11349999904632568\n",
      "Bias: <keras.src.initializers.initializers.RandomNormal object at 0x143f12050>\n",
      "Kernel: <keras.src.initializers.initializers.GlorotNormal object at 0x180abb210>\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3893 - accuracy: 0.8937\n",
      "Test Loss: 0.3892819881439209\n",
      "Test Accuracy: 0.8937000036239624\n",
      "Bias: <keras.src.initializers.initializers.RandomNormal object at 0x143f12050>\n",
      "Kernel: <keras.src.initializers.initializers.HeNormal object at 0x180abae90>\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3762 - accuracy: 0.8967\n",
      "Test Loss: 0.37618860602378845\n",
      "Test Accuracy: 0.8967000246047974\n",
      "Bias: <keras.src.initializers.initializers.Zeros object at 0x145073510>\n",
      "Kernel: <keras.src.initializers.initializers.RandomNormal object at 0x180aba450>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.4433 - accuracy: 0.8788\n",
      "Test Loss: 0.4433404803276062\n",
      "Test Accuracy: 0.8787999749183655\n",
      "Bias: <keras.src.initializers.initializers.Zeros object at 0x145073510>\n",
      "Kernel: <keras.src.initializers.initializers.Zeros object at 0x180abaf90>\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.8816 - accuracy: 0.3112\n",
      "Test Loss: 1.8815808296203613\n",
      "Test Accuracy: 0.31119999289512634\n",
      "Bias: <keras.src.initializers.initializers.Zeros object at 0x145073510>\n",
      "Kernel: <keras.src.initializers.initializers.Ones object at 0x180ab8490>\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3030 - accuracy: 0.0980\n",
      "Test Loss: 2.3030123710632324\n",
      "Test Accuracy: 0.09799999743700027\n",
      "Bias: <keras.src.initializers.initializers.Zeros object at 0x145073510>\n",
      "Kernel: <keras.src.initializers.initializers.GlorotNormal object at 0x180abb210>\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3983 - accuracy: 0.8903\n",
      "Test Loss: 0.39834892749786377\n",
      "Test Accuracy: 0.8902999758720398\n",
      "Bias: <keras.src.initializers.initializers.Zeros object at 0x145073510>\n",
      "Kernel: <keras.src.initializers.initializers.HeNormal object at 0x180abae90>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3915 - accuracy: 0.8970\n",
      "Test Loss: 0.3915303945541382\n",
      "Test Accuracy: 0.8970000147819519\n",
      "Bias: <keras.src.initializers.initializers.Ones object at 0x14707d0d0>\n",
      "Kernel: <keras.src.initializers.initializers.RandomNormal object at 0x180aba450>\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4703 - accuracy: 0.8694\n",
      "Test Loss: 0.470334529876709\n",
      "Test Accuracy: 0.8694000244140625\n",
      "Bias: <keras.src.initializers.initializers.Ones object at 0x14707d0d0>\n",
      "Kernel: <keras.src.initializers.initializers.Zeros object at 0x180abaf90>\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.9798 - accuracy: 0.2609\n",
      "Test Loss: 1.9797574281692505\n",
      "Test Accuracy: 0.26089999079704285\n",
      "Bias: <keras.src.initializers.initializers.Ones object at 0x14707d0d0>\n",
      "Kernel: <keras.src.initializers.initializers.Ones object at 0x180ab8490>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 2.3039 - accuracy: 0.0980\n",
      "Test Loss: 2.303940773010254\n",
      "Test Accuracy: 0.09799999743700027\n",
      "Bias: <keras.src.initializers.initializers.Ones object at 0x14707d0d0>\n",
      "Kernel: <keras.src.initializers.initializers.GlorotNormal object at 0x180abb210>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.4044 - accuracy: 0.8887\n",
      "Test Loss: 0.40435591340065\n",
      "Test Accuracy: 0.888700008392334\n",
      "Bias: <keras.src.initializers.initializers.Ones object at 0x14707d0d0>\n",
      "Kernel: <keras.src.initializers.initializers.HeNormal object at 0x180abae90>\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3947 - accuracy: 0.8956\n",
      "Test Loss: 0.3946733772754669\n",
      "Test Accuracy: 0.8956000208854675\n",
      "Bias: <keras.src.initializers.initializers.GlorotNormal object at 0x180abb010>\n",
      "Kernel: <keras.src.initializers.initializers.RandomNormal object at 0x180aba450>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.4402 - accuracy: 0.8775\n",
      "Test Loss: 0.44022274017333984\n",
      "Test Accuracy: 0.8774999976158142\n",
      "Bias: <keras.src.initializers.initializers.GlorotNormal object at 0x180abb010>\n",
      "Kernel: <keras.src.initializers.initializers.Zeros object at 0x180abaf90>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 1.8518 - accuracy: 0.2982\n",
      "Test Loss: 1.8517919778823853\n",
      "Test Accuracy: 0.29820001125335693\n",
      "Bias: <keras.src.initializers.initializers.GlorotNormal object at 0x180abb010>\n",
      "Kernel: <keras.src.initializers.initializers.Ones object at 0x180ab8490>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 2.3130 - accuracy: 0.1010\n",
      "Test Loss: 2.31296443939209\n",
      "Test Accuracy: 0.10100000351667404\n",
      "Bias: <keras.src.initializers.initializers.GlorotNormal object at 0x180abb010>\n",
      "Kernel: <keras.src.initializers.initializers.GlorotNormal object at 0x180abb210>\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3991 - accuracy: 0.8894\n",
      "Test Loss: 0.39909180998802185\n",
      "Test Accuracy: 0.8894000053405762\n",
      "Bias: <keras.src.initializers.initializers.GlorotNormal object at 0x180abb010>\n",
      "Kernel: <keras.src.initializers.initializers.HeNormal object at 0x180abae90>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3851 - accuracy: 0.8973\n",
      "Test Loss: 0.38511407375335693\n",
      "Test Accuracy: 0.8973000049591064\n",
      "Bias: <keras.src.initializers.initializers.HeNormal object at 0x143f11010>\n",
      "Kernel: <keras.src.initializers.initializers.RandomNormal object at 0x180aba450>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.4623 - accuracy: 0.8712\n",
      "Test Loss: 0.46233975887298584\n",
      "Test Accuracy: 0.8712000250816345\n",
      "Bias: <keras.src.initializers.initializers.HeNormal object at 0x143f11010>\n",
      "Kernel: <keras.src.initializers.initializers.Zeros object at 0x180abaf90>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 1.8991 - accuracy: 0.2618\n",
      "Test Loss: 1.8990929126739502\n",
      "Test Accuracy: 0.26179999113082886\n",
      "Bias: <keras.src.initializers.initializers.HeNormal object at 0x143f11010>\n",
      "Kernel: <keras.src.initializers.initializers.Ones object at 0x180ab8490>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 2.3042 - accuracy: 0.1032\n",
      "Test Loss: 2.304215669631958\n",
      "Test Accuracy: 0.10320000350475311\n",
      "Bias: <keras.src.initializers.initializers.HeNormal object at 0x143f11010>\n",
      "Kernel: <keras.src.initializers.initializers.GlorotNormal object at 0x180abb210>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3946 - accuracy: 0.8919\n",
      "Test Loss: 0.3945685625076294\n",
      "Test Accuracy: 0.8919000029563904\n",
      "Bias: <keras.src.initializers.initializers.HeNormal object at 0x143f11010>\n",
      "Kernel: <keras.src.initializers.initializers.HeNormal object at 0x180abae90>\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3806 - accuracy: 0.8940\n",
      "Test Loss: 0.38059550523757935\n",
      "Test Accuracy: 0.8939999938011169\n"
     ]
    }
   ],
   "source": [
    "# Changing the weight/bias initializations\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "bias_inits = [keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=2002), \n",
    "              keras.initializers.Zeros(), \n",
    "              keras.initializers.Ones(), \n",
    "              keras.initializers.GlorotNormal(seed=2002), \n",
    "              keras.initializers.HeNormal(seed=2002)]\n",
    "\n",
    "weight_inits = [keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=2002), \n",
    "              keras.initializers.Zeros(), \n",
    "              keras.initializers.Ones(), \n",
    "              keras.initializers.GlorotNormal(seed=2002), \n",
    "              keras.initializers.HeNormal(seed=2002)]\n",
    "\n",
    "\n",
    "for i in bias_inits:\n",
    "    for j in weight_inits:\n",
    "        print(\"Bias:\", i)\n",
    "        print(\"Kernel:\", j)\n",
    "        my_nn_model = keras.Sequential()\n",
    "        my_nn_model.add(keras.layers.Flatten(input_shape=(784, 1)))\n",
    "        my_nn_model.add(keras.layers.Dense(128, activation='sigmoid', kernel_initializer=j, bias_initializer=i))\n",
    "        my_nn_model.add(keras.layers.Dense(64, activation='sigmoid', kernel_initializer=j, bias_initializer=i))\n",
    "        my_nn_model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "        # my_nn_model.summary()\n",
    "\n",
    "        my_nn_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "            optimizer=\"sgd\",\n",
    "            metrics=[\"accuracy\"])\n",
    "        # sparse_categorical_crossentropy is used as the loss function since the classified outputs of the mode (0-9) are mutually exclusive\n",
    "\n",
    "        results = my_nn_model.fit(np.array(train_inputs), np.array(train_labels), epochs=10, verbose=False)\n",
    "\n",
    "        test_loss, test_acc = my_nn_model.evaluate(np.array(test_inputs), np.array(test_labels))\n",
    "        print(\"Test Loss:\", test_loss)\n",
    "        print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_12 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109386 (427.29 KB)\n",
      "Trainable params: 109386 (427.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.9691\n",
      "Optimizer used: <keras.src.optimizers.adam.Adam object at 0x1191e7e10> . Learning rate used: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "Test Loss: 0.11039600521326065\n",
      "Test Accuracy: 0.9690999984741211\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0954 - accuracy: 0.9730\n",
      "Optimizer used: <keras.src.optimizers.sgd.SGD object at 0x144516f50> . Learning rate used: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "Test Loss: 0.09540656208992004\n",
      "Test Accuracy: 0.9729999899864197\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1085 - accuracy: 0.9770\n",
      "Optimizer used: <keras.src.optimizers.adam.Adam object at 0x144f88f90> . Learning rate used: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Test Loss: 0.10851398855447769\n",
      "Test Accuracy: 0.9769999980926514\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1084 - accuracy: 0.9770\n",
      "Optimizer used: <keras.src.optimizers.sgd.SGD object at 0x14503afd0> . Learning rate used: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Test Loss: 0.10840463638305664\n",
      "Test Accuracy: 0.9769999980926514\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1141 - accuracy: 0.9766\n",
      "Optimizer used: <keras.src.optimizers.adam.Adam object at 0x1445cfa10> . Learning rate used: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Test Loss: 0.1140570417046547\n",
      "Test Accuracy: 0.9765999913215637\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1141 - accuracy: 0.9767\n",
      "Optimizer used: <keras.src.optimizers.sgd.SGD object at 0x119586450> . Learning rate used: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Test Loss: 0.11405754089355469\n",
      "Test Accuracy: 0.9767000079154968\n"
     ]
    }
   ],
   "source": [
    "# Changing learning rates and optimizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "my_nn_model = keras.Sequential()\n",
    "my_nn_model.add(keras.layers.Flatten(input_shape=(784, 1)))\n",
    "my_nn_model.add(keras.layers.Dense(128, activation='sigmoid'))\n",
    "my_nn_model.add(keras.layers.Dense(64, activation='sigmoid'))\n",
    "my_nn_model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "my_nn_model.summary()\n",
    "\n",
    "__optimizer = []\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "for i in learning_rates:\n",
    "    __optimizer.append(tf.keras.optimizers.Adam(learning_rate=i))\n",
    "    __optimizer.append(tf.keras.optimizers.SGD(learning_rate=i))\n",
    "\n",
    "\n",
    "for i in __optimizer:\n",
    "    my_nn_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=i,\n",
    "                metrics=[\"accuracy\"])\n",
    "    # sparse_categorical_crossentropy is used as the loss function since the classified outputs of the mode (0-9) are mutually exclusive\n",
    "\n",
    "    results = my_nn_model.fit(np.array(train_inputs), np.array(train_labels), epochs=10, verbose=False)\n",
    "\n",
    "    test_loss, test_acc = my_nn_model.evaluate(np.array(test_inputs), np.array(test_labels))\n",
    "    print(\"Optimizer used:\", i, \". Learning rate used:\", i.learning_rate)\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_62 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_186 (Dense)           (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_187 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_188 (Dense)           (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669706 (2.55 MB)\n",
      "Trainable params: 669706 (2.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model training params:\n",
      "\tNumEpochs:  10\n",
      "\tHidden Layer Nodes (512, 512)\n",
      "\tBias Init: <keras.src.initializers.initializers.RandomNormal object at 0x18c244590>\n",
      "\tKernel Init: <keras.src.initializers.initializers.RandomNormal object at 0x18c09f790>\n",
      "Optimizer \n",
      "\tOptimizer used: <keras.src.optimizers.adam.Adam object at 0x185ebc810> . Learning rate used: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mOptimizer used:\u001b[39m\u001b[38;5;124m\"\u001b[39m, i[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Learning rate used:\u001b[39m\u001b[38;5;124m\"\u001b[39m, i[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[0;32m---> 69\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmy_nn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m my_nn_model\u001b[38;5;241m.\u001b[39mevaluate(np\u001b[38;5;241m.\u001b[39marray(test_inputs), np\u001b[38;5;241m.\u001b[39marray(test_labels))\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_loss)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Combining all the changes to the models above, find the model with the best test accuracy:\n",
    "\n",
    "# Will save all data to a file too:\n",
    "\n",
    "from itertools import product\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "\n",
    "\n",
    "\n",
    "__optimizer = []\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "for i in learning_rates:\n",
    "    __optimizer.append(tf.keras.optimizers.Adam(learning_rate=i))\n",
    "    __optimizer.append(tf.keras.optimizers.SGD(learning_rate=i))\n",
    "\n",
    "\n",
    "bias_inits = [keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=2002), \n",
    "              keras.initializers.Zeros(), \n",
    "              keras.initializers.Ones(), \n",
    "              keras.initializers.GlorotNormal(seed=2002), \n",
    "              keras.initializers.HeNormal(seed=2002)]\n",
    "\n",
    "weight_inits = [keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=2002), \n",
    "              keras.initializers.Zeros(), \n",
    "              keras.initializers.Ones(), \n",
    "              keras.initializers.GlorotNormal(seed=2002), \n",
    "              keras.initializers.HeNormal(seed=2002)]\n",
    "\n",
    "\n",
    "num_epochs = [10, 20, 50, 100, 250] # 500 epochs was removed because it would take too long to run with all these other combinations\n",
    "\n",
    "\n",
    "hiddenLayer_numNodes = [512, 256, 128, 64, 32]\n",
    "hiddenLayer_numNodes_pairs = []\n",
    "for i in range(len(hiddenLayer_numNodes)):\n",
    "    for j in range(i, len(hiddenLayer_numNodes)):\n",
    "        # print(\"Nodes in first hidden layer:\", hiddenLayer_numNodes[i], \" Nodes in second hidden layer\", hiddenLayer_numNodes[j])\n",
    "        hiddenLayer_numNodes_pairs.append((hiddenLayer_numNodes[i], hiddenLayer_numNodes[j])) # len = n(n+1) / 2\n",
    "\n",
    "\n",
    "combinations = list(product(__optimizer, bias_inits, weight_inits, num_epochs, hiddenLayer_numNodes_pairs))\n",
    "# Order of combination - optimizer, bias init, weight init, num epochs, hiddenLayer numNodes pairs\n",
    "\n",
    "for i in combinations:\n",
    "    my_nn_model = keras.Sequential()\n",
    "    my_nn_model.add(keras.layers.Flatten(input_shape=(784, 1)))\n",
    "    my_nn_model.add(keras.layers.Dense(i[4][0], activation='sigmoid', kernel_initializer=i[2], bias_initializer=i[1]))\n",
    "    my_nn_model.add(keras.layers.Dense(i[4][1], activation='sigmoid', kernel_initializer=i[2], bias_initializer=i[1]))\n",
    "    my_nn_model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    my_nn_model.summary()\n",
    "\n",
    "\n",
    "    my_nn_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=i[0],\n",
    "                metrics=[\"accuracy\"])\n",
    "    # sparse_categorical_crossentropy is used as the loss function since the classified outputs of the mode (0-9) are mutually exclusive\n",
    "\n",
    "    print(\"Model training params:\")\n",
    "    print(\"\\tNumEpochs: \", i[3])\n",
    "    print(\"\\tHidden Layer Nodes\", i[4])\n",
    "    print(\"\\tBias Init:\", i[1])\n",
    "    print(\"\\tKernel Init:\", i[2])\n",
    "    print(\"Optimizer \")\n",
    "    print(\"\\tOptimizer used:\", i[0], \". Learning rate used:\", i[0].learning_rate)\n",
    "\n",
    "    results = my_nn_model.fit(np.array(train_inputs), np.array(train_labels), epochs=i[3], verbose=False)\n",
    "\n",
    "    test_loss, test_acc = my_nn_model.evaluate(np.array(test_inputs), np.array(test_labels))\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
